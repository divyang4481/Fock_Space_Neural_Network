{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a862710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "\n",
      "=== Baseline ResNet-20 ===\n",
      "epoch 1: train_loss=1.6870\n",
      "Baseline: params=272,474  acc=0.4815  p50=3.24 ms  p90=4.66 ms\n",
      "\n",
      "=== FSNN-MoM ResNet-20 (stage-3 only, M=6, k=2) ===\n",
      "epoch 1: train_loss=1.5476\n",
      "FSNN   : params=1,301,932  acc=0.5495  p50=9.02 ms  p90=13.68 ms\n",
      "\n",
      "=== Summary ===\n",
      "Baseline -> params=272,474, acc=0.4815, p50=3.24 ms\n",
      "FSNN     -> params=1,301,932, acc=0.5495, p50=9.02 ms  (M=6, k=2)\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10: Baseline ResNet-20 vs FSNN-MoM block in stage-3 (channels=64)\n",
    "# FSNN computes only k of M expert residual branches per block (global top-k per batch).\n",
    "import time, random, numpy as np\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "def set_seed(s=42):\n",
    "    random.seed(s); np.random.seed(s)\n",
    "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "set_seed(42)\n",
    "\n",
    "# ---------- data ----------\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "transform_test = transforms.Compose([transforms.ToTensor()])\n",
    "train_ds = datasets.CIFAR10(\"./data\", train=True, download=True, transform=transform_train)\n",
    "test_ds  = datasets.CIFAR10(\"./data\", train=False, download=True, transform=transform_test)\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# ---------- utils ----------\n",
    "def count_params(m): return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "\n",
    "@torch.no_grad()\n",
    "def bs1_latency(model, shape=(1,3,32,32), iters=200, warmup=50):\n",
    "    model.eval()\n",
    "    x = torch.randn(*shape, device=device)\n",
    "    # warmup\n",
    "    for _ in range(warmup):\n",
    "        _ = model(x)\n",
    "        if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "    times = []\n",
    "    for _ in range(iters):\n",
    "        if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "        _ = model(x)\n",
    "        if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "        times.append(time.perf_counter() - t0)\n",
    "    a = np.array(times)\n",
    "    return float(np.median(a)), float(np.percentile(a,90)), float(np.mean(a))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    correct, total, loss_sum = 0, 0, 0.0\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss_sum += F.cross_entropy(logits, yb, reduction=\"sum\").item()\n",
    "        pred = logits.argmax(1)\n",
    "        correct += (pred==yb).sum().item()\n",
    "        total += yb.numel()\n",
    "    return loss_sum/total, correct/total\n",
    "\n",
    "def train(model, epochs=1, lr=0.1, wd=5e-4):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd, nesterov=True)\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs*len(train_loader))\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        run, n = 0.0, 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = F.cross_entropy(logits, yb)\n",
    "            loss.backward()\n",
    "            opt.step(); sched.step()\n",
    "            run += loss.item()*xb.size(0); n += xb.size(0)\n",
    "        print(f\"epoch {ep+1}: train_loss={run/n:.4f}\")\n",
    "\n",
    "# ---------- model pieces ----------\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn1   = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2   = nn.BatchNorm2d(planes)\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes))\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.act(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out = self.act(out + identity)\n",
    "        return out\n",
    "\n",
    "class Router(nn.Module):\n",
    "    \"\"\"Global-average-pooled router -> logits over M experts (one routing for whole batch).\"\"\"\n",
    "    def __init__(self, channels, M):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(channels, M)\n",
    "        nn.init.xavier_uniform_(self.fc.weight); nn.init.zeros_(self.fc.bias)\n",
    "    def forward(self, x):\n",
    "        # x: [B,C,H,W] -> GAP over H,W -> [B,C] -> batch-mean -> [C] -> logits [M]\n",
    "        b, c, h, w = x.shape\n",
    "        z = x.mean(dim=[2,3])              # [B, C]\n",
    "        z = z.mean(dim=0, keepdim=True)    # [1, C] global routing for the batch (fast path)\n",
    "        return self.fc(z)                  # [1, M]\n",
    "\n",
    "class ExpertBlock(nn.Module):\n",
    "    \"\"\"One expert residual branch (like BasicBlock) with given in/out and stride.\"\"\"\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn1   = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2   = nn.BatchNorm2d(planes)\n",
    "        self.act   = nn.ReLU(inplace=True)\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes))\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.act(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        return out + identity              # no final ReLU: FSNN block will add and relu\n",
    "\n",
    "class FSNNMoMBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    FSNN module-of-modules residual block:\n",
    "      - M experts; router picks global top-k; we compute only those experts\n",
    "      - weighted sum of selected experts' outputs, then ReLU\n",
    "    \"\"\"\n",
    "    def __init__(self, in_planes, planes, stride=1, M=6, k=2):\n",
    "        super().__init__()\n",
    "        assert 1 <= k <= M\n",
    "        self.M, self.k = M, k\n",
    "        self.router = Router(in_planes, M)\n",
    "        self.experts = nn.ModuleList([ExpertBlock(in_planes, planes, stride) for _ in range(M)])\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.router(x)            # [1, M]\n",
    "        topv, topi = torch.topk(logits, k=self.k, dim=-1)   # [1, k]\n",
    "        weights = torch.softmax(topv, dim=-1).view(-1)      # [k]\n",
    "        y = 0\n",
    "        for j in range(self.k):            # k is small (1â€“2), loop OK\n",
    "            m = int(topi[0, j])\n",
    "            y = y + weights[j] * self.experts[m](x)\n",
    "        return self.act(y)\n",
    "\n",
    "class ResNetCIFAR(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10, fsnn_last_stage=False, M=6, k=2):\n",
    "        super().__init__()\n",
    "        self.in_planes = 16\n",
    "        self.conv1 = conv3x3(3, 16)\n",
    "        self.bn1   = nn.BatchNorm2d(16)\n",
    "        self.act   = nn.ReLU(inplace=True)\n",
    "        # stages\n",
    "        self.layer1 = self._make_layer(block, 16,  layers[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32,  layers[1], stride=2)\n",
    "        # stage-3: optionally FSNN blocks\n",
    "        if fsnn_last_stage:\n",
    "            blocks = []\n",
    "            for i in range(layers[2]):\n",
    "                stride = 2 if i == 0 else 1\n",
    "                blocks.append(FSNNMoMBlock(self.in_planes, 64, stride=stride, M=M, k=k))\n",
    "                self.in_planes = 64\n",
    "            self.layer3 = nn.Sequential(*blocks)\n",
    "        else:\n",
    "            self.layer3 = self._make_layer(block, 64,  layers[2], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc      = nn.Linear(64, num_classes)\n",
    "        nn.init.xavier_uniform_(self.fc.weight); nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(block(self.in_planes, planes, s))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x); x = self.layer2(x); x = self.layer3(x)\n",
    "        x = self.avgpool(x).flatten(1)\n",
    "        return self.fc(x)\n",
    "\n",
    "def resnet20_cifar10():\n",
    "    return ResNetCIFAR(BasicBlock, [3,3,3], num_classes=10, fsnn_last_stage=False)\n",
    "\n",
    "def fsnn_resnet20_cifar10(M=6, k=2):\n",
    "    return ResNetCIFAR(BasicBlock, [3,3,3], num_classes=10, fsnn_last_stage=True, M=M, k=k)\n",
    "\n",
    "# ---------- run ----------\n",
    "print(\"\\n=== Baseline ResNet-20 ===\")\n",
    "baseline = resnet20_cifar10().to(device)\n",
    "train(baseline, epochs=1, lr=0.1)\n",
    "loss_b, acc_b = evaluate(baseline)\n",
    "p50_b, p90_b, _ = bs1_latency(baseline)\n",
    "pb = count_params(baseline)\n",
    "print(f\"Baseline: params={pb:,}  acc={acc_b:.4f}  p50={p50_b*1e3:.2f} ms  p90={p90_b*1e3:.2f} ms\")\n",
    "\n",
    "print(\"\\n=== FSNN-MoM ResNet-20 (stage-3 only, M=6, k=2) ===\")\n",
    "fsnn = fsnn_resnet20_cifar10(M=6, k=2).to(device)\n",
    "train(fsnn, epochs=1, lr=0.1)\n",
    "loss_f, acc_f = evaluate(fsnn)\n",
    "p50_f, p90_f, _ = bs1_latency(fsnn)\n",
    "pf = count_params(fsnn)\n",
    "print(f\"FSNN   : params={pf:,}  acc={acc_f:.4f}  p50={p50_f*1e3:.2f} ms  p90={p90_f*1e3:.2f} ms\")\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"Baseline -> params={pb:,}, acc={acc_b:.4f}, p50={p50_b*1e3:.2f} ms\")\n",
    "print(f\"FSNN     -> params={pf:,}, acc={acc_f:.4f}, p50={p50_f*1e3:.2f} ms  (M={6}, k={2})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21eea65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "=== Baseline (Bottleneck, stage-3) ===\n",
      "epoch 1: train_loss=1.6770\n",
      "Baseline: params=21,370  acc=0.4704  p50=6.54 ms  p90=9.04 ms\n",
      "\n",
      "=== FSNN (Bottleneck MoM, stage-3; shared 1x1; M=4, k=1) ===\n",
      "epoch 1: train_loss=1.7184\n",
      "FSNN   : params=42,598  acc=0.4578  p50=8.99 ms  p90=12.68 ms\n",
      "\n",
      "=== Summary ===\n",
      "Baseline -> params=21,370, acc=0.4704, p50=6.54 ms\n",
      "FSNN     -> params=42,598, acc=0.4578, p50=8.99 ms  (M=4, k=1)\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10: Baseline Bottleneck ResNet-20 vs FSNN Bottleneck (stage-3 only, shared 1x1, top-k=1)\n",
    "import time, random, numpy as np\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "def set_seed(s=42):\n",
    "    random.seed(s); np.random.seed(s)\n",
    "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "set_seed(42)\n",
    "\n",
    "# ------------------ Data ------------------\n",
    "tf_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "tf_test  = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_ds = datasets.CIFAR10(\"./data\", train=True, download=True, transform=tf_train)\n",
    "test_ds  = datasets.CIFAR10(\"./data\", train=False, download=True, transform=tf_test)\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# ------------------ Utils ------------------\n",
    "def count_params(m): return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "\n",
    "@torch.no_grad()\n",
    "def bs1_latency(model, shape=(1,3,32,32), iters=200, warmup=50):\n",
    "    model.eval()\n",
    "    x = torch.randn(*shape, device=device)\n",
    "    for _ in range(warmup):\n",
    "        _ = model(x)\n",
    "        if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "    ts=[]\n",
    "    for _ in range(iters):\n",
    "        if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter(); _ = model(x)\n",
    "        if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "        ts.append(time.perf_counter()-t0)\n",
    "    a = np.array(ts); return float(np.median(a)), float(np.percentile(a,90)), float(np.mean(a))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    correct, total, loss_sum = 0, 0, 0.0\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss_sum += F.cross_entropy(logits, yb, reduction=\"sum\").item()\n",
    "        pred = logits.argmax(1)\n",
    "        correct += (pred==yb).sum().item(); total += yb.numel()\n",
    "    return loss_sum/total, correct/total\n",
    "\n",
    "def train(model, epochs=1, lr=0.1, wd=5e-4):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd, nesterov=True)\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs*len(train_loader))\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        run, n = 0.0, 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = F.cross_entropy(logits, yb)\n",
    "            loss.backward(); opt.step(); sched.step()\n",
    "            run += loss.item()*xb.size(0); n += xb.size(0)\n",
    "        print(f\"epoch {ep+1}: train_loss={run/n:.4f}\")\n",
    "\n",
    "# ------------------ Conv helpers ------------------\n",
    "def conv1x1(in_ch, out_ch, stride=1):\n",
    "    return nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "def conv3x3(in_ch, out_ch, stride=1):\n",
    "    return nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "# ------------------ Blocks ------------------\n",
    "class BottleneckBlock(nn.Module):\n",
    "    \"\"\"Baseline bottleneck: 1x1 reduce -> 3x3 mid -> 1x1 expand (+ optional downsample).\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, stride=1, r=4):\n",
    "        super().__init__()\n",
    "        mid = out_ch // r\n",
    "        self.reduce = conv1x1(in_ch, mid, stride=stride)\n",
    "        self.bn1    = nn.BatchNorm2d(mid)\n",
    "        self.mid    = conv3x3(mid, mid, stride=1)\n",
    "        self.bn2    = nn.BatchNorm2d(mid)\n",
    "        self.expand = conv1x1(mid, out_ch, stride=1)\n",
    "        self.bn3    = nn.BatchNorm2d(out_ch)\n",
    "        self.act    = nn.ReLU(inplace=True)\n",
    "        self.down   = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.down = nn.Sequential(conv1x1(in_ch, out_ch, stride=stride),\n",
    "                                      nn.BatchNorm2d(out_ch))\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        y = self.act(self.bn1(self.reduce(x)))\n",
    "        y = self.act(self.bn2(self.mid(y)))\n",
    "        y = self.bn3(self.expand(y))\n",
    "        if self.down is not None: identity = self.down(x)\n",
    "        return self.act(y + identity)\n",
    "\n",
    "class Router(nn.Module):\n",
    "    \"\"\"Global per-batch router over reduced features (GAP -> linear).\"\"\"\n",
    "    def __init__(self, ch, M):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(ch, M)\n",
    "        nn.init.xavier_uniform_(self.fc.weight); nn.init.zeros_(self.fc.bias)\n",
    "    def forward(self, y_reduced):  # y_reduced: [B, C, H, W]\n",
    "        z = y_reduced.mean(dim=[0,2,3], keepdim=True)  # [1, C]\n",
    "        return self.fc(z.view(1, -1))                  # [1, M]\n",
    "\n",
    "class FSNNBottleneckMoM(nn.Module):\n",
    "    \"\"\"\n",
    "    FSNN bottleneck with shared 1x1 reduce/expand.\n",
    "    Only the 3x3 mid conv is modular: M experts; compute top-1 (k=1).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, stride=1, r=4, M=4, k=1):\n",
    "        super().__init__(); assert 1 <= k <= M\n",
    "        self.k, self.M = k, M\n",
    "        mid = out_ch // r\n",
    "        # Shared reduce/expand\n",
    "        self.reduce = conv1x1(in_ch, mid, stride=stride)\n",
    "        self.bn1    = nn.BatchNorm2d(mid)\n",
    "        self.expand = conv1x1(mid, out_ch, stride=1)\n",
    "        self.bn3    = nn.BatchNorm2d(out_ch)\n",
    "        # Expert 3x3 mids\n",
    "        self.mids = nn.ModuleList([conv3x3(mid, mid, stride=1) for _ in range(M)])\n",
    "        self.mbn  = nn.ModuleList([nn.BatchNorm2d(mid) for _ in range(M)])\n",
    "        # Router on reduced features\n",
    "        self.router = Router(mid, M)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.down = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.down = nn.Sequential(conv1x1(in_ch, out_ch, stride=stride),\n",
    "                                      nn.BatchNorm2d(out_ch))\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        y1 = self.act(self.bn1(self.reduce(x)))      # shared reduce\n",
    "        logits = self.router(y1)                     # [1, M]\n",
    "        topv, topi = torch.topk(logits, k=self.k, dim=-1)\n",
    "        weights = torch.softmax(topv, dim=-1).view(-1)   # [k]\n",
    "        y = 0\n",
    "        for j in range(self.k):                      # k is tiny (1â€“2)\n",
    "            m = int(topi[0, j])\n",
    "            y_mid = self.act(self.mbn[m](self.mids[m](y1)))\n",
    "            y = y + weights[j] * y_mid\n",
    "        y = self.bn3(self.expand(y))                 # shared expand\n",
    "        if self.down is not None: identity = self.down(x)\n",
    "        return self.act(y + identity)\n",
    "\n",
    "# ------------------ ResNet skeleton ------------------\n",
    "class ResNetCIFAR(nn.Module):\n",
    "    def __init__(self, block1, block3, layers=(3,3,3), r=4, fsnn_cfg=None, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_ch = 16\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # stage 1 (16)\n",
    "        self.layer1 = self._make_layer(block1, 16, layers[0], stride=1, r=r)\n",
    "        # stage 2 (32)\n",
    "        self.layer2 = self._make_layer(block1, 32, layers[1], stride=2, r=r)\n",
    "        # stage 3 (64): baseline or FSNN\n",
    "        if fsnn_cfg is None:\n",
    "            self.layer3 = self._make_layer(block3, 64, layers[2], stride=2, r=r)\n",
    "        else:\n",
    "            self.layer3 = self._make_layer_fsnn(64, layers[2], stride=2, r=r, **fsnn_cfg)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc   = nn.Linear(64, num_classes)\n",
    "        nn.init.xavier_uniform_(self.fc.weight); nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def _make_layer(self, block, out_ch, n, stride, r):\n",
    "        layers = []\n",
    "        for i in range(n):\n",
    "            s = stride if i == 0 else 1\n",
    "            layers.append(block(self.in_ch, out_ch, stride=s, r=r))\n",
    "            self.in_ch = out_ch\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_layer_fsnn(self, out_ch, n, stride, r, M=4, k=1):\n",
    "        layers = []\n",
    "        for i in range(n):\n",
    "            s = stride if i == 0 else 1\n",
    "            layers.append(FSNNBottleneckMoM(self.in_ch, out_ch, stride=s, r=r, M=M, k=k))\n",
    "            self.in_ch = out_ch\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x); x = self.layer2(x); x = self.layer3(x)\n",
    "        x = self.pool(x).flatten(1)\n",
    "        return self.fc(x)\n",
    "\n",
    "def resnet20_bottleneck():\n",
    "    return ResNetCIFAR(BottleneckBlock, BottleneckBlock, layers=(3,3,3), r=4)\n",
    "\n",
    "def fsnn_resnet20_bottleneck(M=4, k=1):\n",
    "    return ResNetCIFAR(BottleneckBlock, BottleneckBlock, layers=(3,3,3), r=4,\n",
    "                       fsnn_cfg=dict(M=M, k=k))\n",
    "\n",
    "# ------------------ Run ------------------\n",
    "EPOCHS = 1     # use 50â€“100 for real accuracy\n",
    "M, K = 4, 1    # experts and active experts (compute top-1)\n",
    "\n",
    "print(\"\\n=== Baseline (Bottleneck, stage-3) ===\")\n",
    "baseline = resnet20_bottleneck().to(device)\n",
    "train(baseline, epochs=EPOCHS, lr=0.1)\n",
    "loss_b, acc_b = evaluate(baseline)\n",
    "p50_b, p90_b, _ = bs1_latency(baseline)\n",
    "pb = count_params(baseline)\n",
    "print(f\"Baseline: params={pb:,}  acc={acc_b:.4f}  p50={p50_b*1e3:.2f} ms  p90={p90_b*1e3:.2f} ms\")\n",
    "\n",
    "print(f\"\\n=== FSNN (Bottleneck MoM, stage-3; shared 1x1; M={M}, k={K}) ===\")\n",
    "fsnn = fsnn_resnet20_bottleneck(M=M, k=K).to(device)\n",
    "train(fsnn, epochs=EPOCHS, lr=0.1)\n",
    "loss_f, acc_f = evaluate(fsnn)\n",
    "p50_f, p90_f, _ = bs1_latency(fsnn)\n",
    "pf = count_params(fsnn)\n",
    "print(f\"FSNN   : params={pf:,}  acc={acc_f:.4f}  p50={p50_f*1e3:.2f} ms  p90={p90_f*1e3:.2f} ms\")\n",
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"Baseline -> params={pb:,}, acc={acc_b:.4f}, p50={p50_b*1e3:.2f} ms\")\n",
    "print(f\"FSNN     -> params={pf:,}, acc={acc_f:.4f}, p50={p50_f*1e3:.2f} ms  (M={M}, k={K})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "433af907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "Theory FLOPs ratio FSNN/Baseline â‰ˆ 0.25  (expected ~0.25x compute)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_inductor\\compile_fx.py:167: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "BackendCompilerFailed",
     "evalue": "backend='inductor' raised:\nRuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_dynamo\\output_graph.py:1446\u001b[0m, in \u001b[0;36mOutputGraph._call_user_compiler\u001b[1;34m(self, gm)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     compiler_fn \u001b[38;5;241m=\u001b[39m WrapperBackend(compiler_fn)\n\u001b[1;32m-> 1446\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1447\u001b[0m _step_logger()(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone compiler function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py:129\u001b[0m, in \u001b[0;36mWrapBackendDebug.__call__\u001b[1;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 129\u001b[0m     compiled_gm \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\__init__.py:2234\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[1;34m(self, model_, inputs_)\u001b[0m\n\u001b[0;32m   2232\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_fx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_fx\n\u001b[1;32m-> 2234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompile_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_inductor\\compile_fx.py:1521\u001b[0m, in \u001b[0;36mcompile_fx\u001b[1;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[0m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m V\u001b[38;5;241m.\u001b[39mset_fake_mode(fake_mode), torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mtracing(\n\u001b[0;32m   1517\u001b[0m     tracing_context\n\u001b[0;32m   1518\u001b[0m ), compiled_autograd\u001b[38;5;241m.\u001b[39mdisable(), functorch_config\u001b[38;5;241m.\u001b[39mpatch(\n\u001b[0;32m   1519\u001b[0m     unlift_effect_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1520\u001b[0m ):\n\u001b[1;32m-> 1521\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maot_autograd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1524\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecompositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecompositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartition_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_inference_input_mutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_dynamo\\backends\\common.py:72\u001b[0m, in \u001b[0;36mAotAutograd.__call__\u001b[1;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m enable_aot_logging(), patch_config:\n\u001b[1;32m---> 72\u001b[0m     cg \u001b[38;5;241m=\u001b[39m aot_module_simplified(gm, example_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     73\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_functorch\\aot_autograd.py:1071\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[1;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler, cudagraphs)\u001b[0m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1071\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_and_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mGmWrapper):\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;66;03m# This function is called by the flatten_graph_inputs wrapper, which boxes\u001b[39;00m\n\u001b[0;32m   1075\u001b[0m     \u001b[38;5;66;03m# the inputs so that they can be freed before the end of this scope.\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m     \u001b[38;5;66;03m# For overhead reasons, this is not the default wrapper, see comment:\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/122535/files#r1560096481\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_functorch\\aot_autograd.py:1056\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.dispatch_and_compile\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compiled_autograd\u001b[38;5;241m.\u001b[39mdisable():\n\u001b[1;32m-> 1056\u001b[0m     compiled_fn, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunctional_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m        \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1061\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_functorch\\aot_autograd.py:522\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[1;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_aot_dispatcher_function\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 522\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape_env\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_functorch\\aot_autograd.py:759\u001b[0m, in \u001b[0;36m_create_aot_dispatcher_function\u001b[1;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[0;32m    757\u001b[0m compiler_fn \u001b[38;5;241m=\u001b[39m choose_dispatcher(needs_autograd, aot_config)\n\u001b[1;32m--> 759\u001b[0m compiled_fn, fw_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_dup_fake_script_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    763\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    764\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn, fw_metadata\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py:179\u001b[0m, in \u001b[0;36maot_dispatch_base\u001b[1;34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TracingContext\u001b[38;5;241m.\u001b[39mreport_output_strides() \u001b[38;5;28;01mas\u001b[39;00m fwd_output_strides:\n\u001b[1;32m--> 179\u001b[0m     compiled_fw \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfw_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdated_flat_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fakified_out_wrapper\u001b[38;5;241m.\u001b[39mneeds_post_compile:\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_inductor\\compile_fx.py:1350\u001b[0m, in \u001b[0;36mcompile_fx.<locals>.fw_compiler_base\u001b[1;34m(model, example_inputs, is_inference)\u001b[0m\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_utils\u001b[38;5;241m.\u001b[39mdynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompile_fx.<locals>.fw_compiler_base\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fw_compiler_base\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_inference\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_inductor\\compile_fx.py:1421\u001b[0m, in \u001b[0;36mcompile_fx.<locals>._fw_compiler_base\u001b[1;34m(model, example_inputs, is_inference)\u001b[0m\n\u001b[0;32m   1413\u001b[0m     user_visible_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mfromkeys(\n\u001b[0;32m   1414\u001b[0m         n\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m   1415\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m model_outputs[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(n, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mNode)\n\u001b[0;32m   1419\u001b[0m     )\n\u001b[1;32m-> 1421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_input_idxs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_static_input_idxs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfixed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_inference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboxed_forward_device_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforward_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1429\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_visible_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_visible_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1430\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_inductor\\compile_fx.py:475\u001b[0m, in \u001b[0;36mcompile_fx_inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    473\u001b[0m stack\u001b[38;5;241m.\u001b[39menter_context(DebugContext())\n\u001b[1;32m--> 475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_compiler_debug(_compile_fx_inner, compiler_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minductor\u001b[39m\u001b[38;5;124m\"\u001b[39m)(\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    477\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py:85\u001b[0m, in \u001b[0;36mwrap_compiler_debug.<locals>.debug_wrapper\u001b[1;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;66;03m# Call the compiler_fn - which is either aot_autograd or inductor\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# with fake inputs\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     inner_compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# TODO: Failures here are troublesome because no real inputs,\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;66;03m# need a different serialization strategy\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_inductor\\compile_fx.py:661\u001b[0m, in \u001b[0;36m_compile_fx_inner\u001b[1;34m(gm, example_inputs, cudagraphs, static_input_idxs, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, boxed_forward_device_index, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[0;32m    659\u001b[0m             \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39m_is_inductor_static \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 661\u001b[0m     compiled_graph \u001b[38;5;241m=\u001b[39m \u001b[43mFxGraphCache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcodegen_and_compile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgraph_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    666\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfx_graph_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfx_graph_remote_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_inductor\\codecache.py:1334\u001b[0m, in \u001b[0;36mFxGraphCache.load\u001b[1;34m(compile_fx_fn, gm, example_inputs, fx_kwargs, inputs_to_check, local, remote)\u001b[0m\n\u001b[0;32m   1333\u001b[0m cache_event_time \u001b[38;5;241m=\u001b[39m start_time\n\u001b[1;32m-> 1334\u001b[0m compiled_graph \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_fx_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfx_kwargs\u001b[49m\n\u001b[0;32m   1336\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1337\u001b[0m compiled_graph\u001b[38;5;241m.\u001b[39m_time_taken_ns \u001b[38;5;241m=\u001b[39m time_ns() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_inductor\\compile_fx.py:570\u001b[0m, in \u001b[0;36m_compile_fx_inner.<locals>.codegen_and_compile\u001b[1;34m(gm, example_inputs, inputs_to_check, fx_kwargs)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;124;03mThis function calls fx_codegen_and_compile and also adds some extra metadata to the resulting\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;124;03mcompiled fx graph. The metadata is saved to FXGraphCache.\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m compiled_graph \u001b[38;5;241m=\u001b[39m fx_codegen_and_compile(gm, example_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfx_kwargs)\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(compiled_graph, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;66;03m# We only return a string in aot mode, in which case we don't\u001b[39;00m\n\u001b[0;32m    573\u001b[0m     \u001b[38;5;66;03m# need to do any post-compilation steps: we just return the string,\u001b[39;00m\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;66;03m# which is the filename of the compiled code.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_inductor\\compile_fx.py:878\u001b[0m, in \u001b[0;36mfx_codegen_and_compile\u001b[1;34m(gm, example_inputs, cudagraphs, static_input_idxs, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[0;32m    877\u001b[0m _check_triton_bf16_support(graph)\n\u001b[1;32m--> 878\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_to_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    879\u001b[0m num_bytes, nodes_num_elem, node_runtimes \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mcount_bytes()\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_inductor\\graph.py:1913\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_fn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1912\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1913\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcall\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_inductor\\graph.py:1839\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1836\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[0;32m   1837\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraphLowering.compile_to_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_gen\u001b[39m\u001b[38;5;124m\"\u001b[39m, fwd_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1838\u001b[0m ):\n\u001b[1;32m-> 1839\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_inductor\\graph.py:1845\u001b[0m, in \u001b[0;36mGraphLowering._compile_to_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1842\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcodecache\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PyCodeCache\n\u001b[0;32m   1844\u001b[0m code, linemap \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1845\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodegen_with_cpp_wrapper() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpp_wrapper \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1846\u001b[0m )\n\u001b[0;32m   1848\u001b[0m GraphLowering\u001b[38;5;241m.\u001b[39msave_output_code(code)\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_inductor\\graph.py:1780\u001b[0m, in \u001b[0;36mGraphLowering.codegen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1778\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_wrapper_code()\n\u001b[1;32m-> 1780\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;241m=\u001b[39m \u001b[43mScheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1781\u001b[0m V\u001b[38;5;241m.\u001b[39mdebug\u001b[38;5;241m.\u001b[39mdraw_orig_fx_graph(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_gm, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mnodes)\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_inductor\\scheduler.py:1731\u001b[0m, in \u001b[0;36mScheduler.__init__\u001b[1;34m(self, nodes)\u001b[0m\n\u001b[0;32m   1730\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScheduler.__init__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1731\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_inductor\\scheduler.py:1749\u001b[0m, in \u001b[0;36mScheduler._init\u001b[1;34m(self, nodes)\u001b[0m\n\u001b[0;32m   1741\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable_buffer_names \u001b[38;5;241m=\u001b[39m OrderedSet(\n\u001b[0;32m   1742\u001b[0m     [\n\u001b[0;32m   1743\u001b[0m         \u001b[38;5;241m*\u001b[39mV\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mgraph_inputs\u001b[38;5;241m.\u001b[39mkeys(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1746\u001b[0m     ]\n\u001b[0;32m   1747\u001b[0m )\n\u001b[1;32m-> 1749\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_scheduler_node(n) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes]\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_zero_dim_cpu_tensor()\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_inductor\\scheduler.py:1749\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1741\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable_buffer_names \u001b[38;5;241m=\u001b[39m OrderedSet(\n\u001b[0;32m   1742\u001b[0m     [\n\u001b[0;32m   1743\u001b[0m         \u001b[38;5;241m*\u001b[39mV\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mgraph_inputs\u001b[38;5;241m.\u001b[39mkeys(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1746\u001b[0m     ]\n\u001b[0;32m   1747\u001b[0m )\n\u001b[1;32m-> 1749\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_scheduler_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes]\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_zero_dim_cpu_tensor()\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_inductor\\scheduler.py:1856\u001b[0m, in \u001b[0;36mScheduler.create_scheduler_node\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, (ir\u001b[38;5;241m.\u001b[39mComputedBuffer, ir\u001b[38;5;241m.\u001b[39mTemplateBuffer)):\n\u001b[1;32m-> 1856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSchedulerNode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, ir\u001b[38;5;241m.\u001b[39mExternKernel):\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_inductor\\scheduler.py:833\u001b[0m, in \u001b[0;36mSchedulerNode.__init__\u001b[1;34m(self, scheduler, node)\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_from_node(node)\n\u001b[1;32m--> 833\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_attrs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_inductor\\scheduler.py:846\u001b[0m, in \u001b[0;36mSchedulerNode._compute_attrs\u001b[1;34m(self, extra_indexing_constraints, recompute_sizes_body_func)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sizes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39msimplify_and_reorder(\n\u001b[0;32m    842\u001b[0m     extra_indexing_constraints\u001b[38;5;241m=\u001b[39mextra_indexing_constraints,\n\u001b[0;32m    843\u001b[0m     recompute_sizes_body_func\u001b[38;5;241m=\u001b[39mrecompute_sizes_body_func,\n\u001b[0;32m    844\u001b[0m )\n\u001b[1;32m--> 846\u001b[0m group_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgroup_fn\n\u001b[0;32m    847\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mget_device(), group_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sizes))\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_inductor\\scheduler.py:3360\u001b[0m, in \u001b[0;36mScheduler.get_backend\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackends:\n\u001b[1;32m-> 3360\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackends[device] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackends[device]\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_inductor\\scheduler.py:3352\u001b[0m, in \u001b[0;36mScheduler.create_backend\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m   3351\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m is_gpu(device\u001b[38;5;241m.\u001b[39mtype):\n\u001b[1;32m-> 3352\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   3353\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa: B950\u001b[39;00m\n\u001b[0;32m   3354\u001b[0m         )\n\u001b[0;32m   3356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m device_scheduling(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 92\u001b[0m\n\u001b[0;32m     89\u001b[0m ffn_fsnn \u001b[38;5;241m=\u001b[39m try_compile(FFN_MoM_TopK(d\u001b[38;5;241m=\u001b[39md, M\u001b[38;5;241m=\u001b[39mM, k\u001b[38;5;241m=\u001b[39mk, h\u001b[38;5;241m=\u001b[39mh)\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# --- measure ---\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m p50b, p90b, _ \u001b[38;5;241m=\u001b[39m \u001b[43mbs1_latency\u001b[49m\u001b[43m(\u001b[49m\u001b[43mffn_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m p50f, p90f, _ \u001b[38;5;241m=\u001b[39m bs1_latency(ffn_fsnn, x)\n\u001b[0;32m     94\u001b[0m pb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m ffn_base\u001b[38;5;241m.\u001b[39mparameters())\n",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m, in \u001b[0;36mbs1_latency\u001b[1;34m(model, x, iters, warmup)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(warmup):\n\u001b[1;32m---> 12\u001b[0m         _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[0;32m     14\u001b[0m     ts\u001b[38;5;241m=\u001b[39m[]\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:465\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    460\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    461\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[0;32m    462\u001b[0m )\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[0;32m    469\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[0;32m    470\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1269\u001b[0m, in \u001b[0;36mCatchErrorsWrapper.__call__\u001b[1;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[0;32m   1263\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(\n\u001b[0;32m   1264\u001b[0m                 frame, cache_entry, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks, frame_state\n\u001b[0;32m   1265\u001b[0m             )\n\u001b[0;32m   1267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[0;32m   1268\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[1;32m-> 1269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1064\u001b[0m, in \u001b[0;36mConvertFrame.__call__\u001b[1;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[0;32m   1062\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1064\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1067\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1068\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:526\u001b[0m, in \u001b[0;36mConvertFrameAssert.__call__\u001b[1;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[0;32m    510\u001b[0m compile_id \u001b[38;5;241m=\u001b[39m CompileId(frame_id, frame_compile_id)\n\u001b[0;32m    512\u001b[0m signpost_event(\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_frame_assert._compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m     },\n\u001b[0;32m    524\u001b[0m )\n\u001b[1;32m--> 526\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:924\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[0;32m    922\u001b[0m guarded_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 924\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m guarded_code\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:666\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[1;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_compile.compile_inner\u001b[39m\u001b[38;5;124m\"\u001b[39m, phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentire_frame_compile\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    665\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m CompileTimeInstructionCounter\u001b[38;5;241m.\u001b[39mrecord():\n\u001b[1;32m--> 666\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_utils_internal.py:87\u001b[0m, in \u001b[0;36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39mprofile_compile_time(\n\u001b[0;32m     90\u001b[0m     function, phase_name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m     91\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:699\u001b[0m, in \u001b[0;36m_compile.<locals>._compile_inner\u001b[1;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[0;32m    697\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 699\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py:1322\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[1;34m(code, transformations, safe)\u001b[0m\n\u001b[0;32m   1319\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[0;32m   1320\u001b[0m propagate_line_nums(instructions)\n\u001b[1;32m-> 1322\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:219\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m exit_stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[0;32m    216\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39m_symbolic_trace\u001b[38;5;241m.\u001b[39m_maybe_revert_all_patches()\n\u001b[0;32m    217\u001b[0m )\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    221\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:634\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[1;34m(instructions, code_options)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[1;32m--> 634\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[0;32m    636\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:2796\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 2796\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:983\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    982\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 983\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    984\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:895\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 895\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[0;32m    897\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:2987\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[1;34m(self, inst)\u001b[0m\n\u001b[0;32m   2986\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mRETURN_VALUE\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[1;32m-> 2987\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:2972\u001b[0m, in \u001b[0;36mInstructionTranslator._return\u001b[1;34m(self, inst)\u001b[0m\n\u001b[0;32m   2967\u001b[0m _step_logger()(\n\u001b[0;32m   2968\u001b[0m     logging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[0;32m   2969\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchdynamo done tracing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;241m.\u001b[39mopname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2970\u001b[0m )\n\u001b[0;32m   2971\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m triggered compile\u001b[39m\u001b[38;5;124m\"\u001b[39m, inst\u001b[38;5;241m.\u001b[39mopname)\n\u001b[1;32m-> 2972\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_subgraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2973\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGraphCompileReason\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreturn_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   2976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2977\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2978\u001b[0m return_inst \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2979\u001b[0m     create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2980\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2981\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_CONST\u001b[39m\u001b[38;5;124m\"\u001b[39m, argval\u001b[38;5;241m=\u001b[39minst\u001b[38;5;241m.\u001b[39margval)\n\u001b[0;32m   2982\u001b[0m )\n\u001b[0;32m   2983\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39madd_output_instructions([return_inst])\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_dynamo\\output_graph.py:1117\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[1;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[0;32m   1114\u001b[0m append_prefix_insts()\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;66;03m# optimization to generate better code in a common case\u001b[39;00m\n\u001b[0;32m   1116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_output_instructions(\n\u001b[1;32m-> 1117\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_and_call_fx_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mreversed\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstack_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;241m+\u001b[39m [create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNPACK_SEQUENCE\u001b[39m\u001b[38;5;124m\"\u001b[39m, arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(stack_values))]\n\u001b[0;32m   1119\u001b[0m )\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;66;03m# restore all the live local vars\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_output_instructions(\n\u001b[0;32m   1122\u001b[0m     [PyCodegen(tx)\u001b[38;5;241m.\u001b[39mcreate_store(var) \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(restore_vars)]\n\u001b[0;32m   1123\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_dynamo\\output_graph.py:1369\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[1;34m(self, tx, rv, root)\u001b[0m\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracing_context\u001b[38;5;241m.\u001b[39mfake_mode \u001b[38;5;241m=\u001b[39m backend_fake_mode\n\u001b[0;32m   1368\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_global_state():\n\u001b[1;32m-> 1369\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lazy_graph_module\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LazyGraphModule\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(compiled_fn, _LazyGraphModule) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1374\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(compiled_fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), _LazyGraphModule)\n\u001b[0;32m   1375\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m compiled_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_lazy_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;66;03m# this is a _LazyGraphModule. This makes it easier for dynamo to\u001b[39;00m\n\u001b[0;32m   1380\u001b[0m     \u001b[38;5;66;03m# optimize a _LazyGraphModule.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_dynamo\\output_graph.py:1416\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[1;34m(self, gm)\u001b[0m\n\u001b[0;32m   1412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_user_compiler\u001b[39m(\u001b[38;5;28mself\u001b[39m, gm: fx\u001b[38;5;241m.\u001b[39mGraphModule) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CompiledFn:\n\u001b[0;32m   1413\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[0;32m   1414\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputGraph.call_user_compiler\u001b[39m\u001b[38;5;124m\"\u001b[39m, phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackend_compile\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1415\u001b[0m     ):\n\u001b[1;32m-> 1416\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\nvidia_env\\lib\\site-packages\\torch\\_dynamo\\output_graph.py:1465\u001b[0m, in \u001b[0;36mOutputGraph._call_user_compiler\u001b[1;34m(self, gm)\u001b[0m\n\u001b[0;32m   1463\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1465\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BackendCompilerFailed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiler_fn, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1467\u001b[0m signpost_event(\n\u001b[0;32m   1468\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1469\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputGraph.call_user_compiler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1475\u001b[0m     },\n\u001b[0;32m   1476\u001b[0m )\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "\u001b[1;31mBackendCompilerFailed\u001b[0m: backend='inductor' raised:\nRuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "# Transformer FFN microbench: Baseline vs FSNN-MoM (top-k) with torch.compile + theory FLOP ratios\n",
    "import time, math, numpy as np, torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "# --- helpers ---\n",
    "def bs1_latency(model, x, iters=400, warmup=80):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            _ = model(x)\n",
    "            if device.type==\"cuda\": torch.cuda.synchronize()\n",
    "        ts=[]\n",
    "        for _ in range(iters):\n",
    "            if device.type==\"cuda\": torch.cuda.synchronize()\n",
    "            t0=time.perf_counter()\n",
    "            _=model(x)\n",
    "            if device.type==\"cuda\": torch.cuda.synchronize()\n",
    "            ts.append(time.perf_counter()-t0)\n",
    "    a=np.array(ts); return float(np.median(a)), float(np.percentile(a,90)), float(np.mean(a))\n",
    "\n",
    "def try_compile(m):\n",
    "    try:\n",
    "        return torch.compile(m)  # PyTorch 2+\n",
    "    except Exception:\n",
    "        return m\n",
    "\n",
    "# --- models ---\n",
    "class FFN(nn.Module):\n",
    "    # Baseline Transformer FFN: d -> d_ff -> d\n",
    "    def __init__(self, d=512, d_ff=2048):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d)\n",
    "        self.act = nn.GELU()\n",
    "    def forward(self, x):          # x: [B, T, d]\n",
    "        y = self.act(self.fc1(x))\n",
    "        return self.fc2(y)\n",
    "\n",
    "class FFN_MoM_TopK(nn.Module):\n",
    "    \"\"\"\n",
    "    FSNN Mixture-of-Modules FFN:\n",
    "      - M experts, each d->h->d\n",
    "      - router runs on pooled tokens (global per batch) -> pick top-k experts\n",
    "      - compute ONLY those k experts, weighted sum\n",
    "      - dormant capacity: M*h ~= baseline d_ff\n",
    "      - active compute:   k*h  << baseline d_ff\n",
    "    \"\"\"\n",
    "    def __init__(self, d=512, M=8, k=2, h=256):\n",
    "        super().__init__(); assert 1<=k<=M\n",
    "        self.M, self.k, self.d, self.h = M, k, d, h\n",
    "        self.W1 = nn.Parameter(torch.empty(M, d, h));   self.b1 = nn.Parameter(torch.zeros(M, h))\n",
    "        self.W2 = nn.Parameter(torch.empty(M, h, d));   self.b2 = nn.Parameter(torch.zeros(M, d))\n",
    "        nn.init.xavier_uniform_(self.W1); nn.init.xavier_uniform_(self.W2)\n",
    "        self.router = nn.Linear(d, M)\n",
    "        nn.init.xavier_uniform_(self.router.weight); nn.init.zeros_(self.router.bias)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):          # x: [B, T, d]\n",
    "        B,T,d = x.shape\n",
    "        g = x.mean(dim=[0,1], keepdim=True)     # [1,1,d] pooled\n",
    "        logits = self.router(g.view(1,d))       # [1,M]\n",
    "        topv, topi = torch.topk(logits, k=self.k, dim=-1)  # [1,k]\n",
    "        w = torch.softmax(topv, dim=-1).view(-1)           # [k]\n",
    "        y = 0\n",
    "        # k is tiny (1â€“2), so looping is OK; we still do large fused matmuls\n",
    "        for j in range(self.k):\n",
    "            m = int(topi[0, j])\n",
    "            y1 = self.act(torch.matmul(x, self.W1[m]) + self.b1[m])   # [B,T,h]\n",
    "            y  = y + w[j] * (torch.matmul(y1, self.W2[m]) + self.b2[m])  # [B,T,d]\n",
    "        return y\n",
    "\n",
    "# --- shapes & configs ---\n",
    "B, T, d = 1, 128, 512\n",
    "d_ff = 2048                   # baseline inner width\n",
    "M, k, h = 8, 2, 256           # FSNN: dormant = M*h = 2048 (same), active = k*h = 512 (4Ã— smaller)\n",
    "x = torch.randn(B, T, d, device=device)\n",
    "\n",
    "# Theoretical FLOPs (ignoring bias/activation):\n",
    "# Baseline ~ 2 * B * T * d * d_ff\n",
    "# FSNN    ~ 2 * B * T * d * (k*h)\n",
    "flops_base = 2*B*T*d*d_ff\n",
    "flops_fsnn = 2*B*T*d*(k*h)\n",
    "print(f\"Theory FLOPs ratio FSNN/Baseline â‰ˆ {flops_fsnn/flops_base:.2f}  (expected ~{(k*h)/d_ff:.2f}x compute)\")\n",
    "\n",
    "# --- build & (optionally) compile ---\n",
    "ffn_base = try_compile(FFN(d=d, d_ff=d_ff).to(device))\n",
    "ffn_fsnn = try_compile(FFN_MoM_TopK(d=d, M=M, k=k, h=h).to(device))\n",
    "\n",
    "# --- measure ---\n",
    "p50b, p90b, _ = bs1_latency(ffn_base, x)\n",
    "p50f, p90f, _ = bs1_latency(ffn_fsnn, x)\n",
    "pb = sum(p.numel() for p in ffn_base.parameters())\n",
    "pf = sum(p.numel() for p in ffn_fsnn.parameters())\n",
    "\n",
    "print(f\"\\nBaseline FFN: params={pb:,}  d_ff={d_ff}  p50={p50b*1e3:.2f} ms  p90={p90b*1e3:.2f} ms\")\n",
    "print(f\"FSNN-MoM    : params={pf:,}  M={M},k={k},h={h}  (active {k*h}, dormant {M*h})\")\n",
    "print(f\"              p50={p50f*1e3:.2f} ms  p90={p90f*1e3:.2f} ms  -> speedup ~{p50b/max(p50f,1e-9):.2f}Ã—\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvidia_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
