{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8ac2e4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# FSNN top-k (fixed gather) â€” Param-parity vs Compute-parity on MNIST\n",
    "import time, random, math, numpy as np\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "# ----- data -----\n",
    "BATCH_SIZE = 128\n",
    "D = 28*28; NUM_CLASSES = 10\n",
    "transform = transforms.ToTensor()\n",
    "train_ds = datasets.MNIST(\"./data\", train=True,  download=True, transform=transform)\n",
    "test_ds  = datasets.MNIST(\"./data\", train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "def count_params(m): return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "\n",
    "@torch.no_grad()\n",
    "def latency_bs1(model, d=D, iters=120, warmup=30):\n",
    "    model.eval()\n",
    "    x = torch.randn(1, d, device=device)\n",
    "    for _ in range(warmup):\n",
    "        _ = model(x);  torch.cuda.synchronize() if device.type==\"cuda\" else None\n",
    "    ts=[]\n",
    "    for _ in range(iters):\n",
    "        torch.cuda.synchronize() if device.type==\"cuda\" else None\n",
    "        t0=time.perf_counter(); _=model(x)\n",
    "        torch.cuda.synchronize() if device.type==\"cuda\" else None\n",
    "        ts.append(time.perf_counter()-t0)\n",
    "    ts=np.array(ts);  return float(np.median(ts)), float(np.percentile(ts,90)), float(np.mean(ts))\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    correct, total, loss_sum = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.view(xb.size(0), -1).to(device); yb = yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss_sum += F.cross_entropy(logits, yb, reduction=\"sum\").item()\n",
    "            pred = logits.argmax(1); correct += (pred==yb).sum().item(); total += yb.numel()\n",
    "    return loss_sum/total, correct/total\n",
    "\n",
    "def train(model, epochs=2, lr=1e-3, wd=0.0):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    model.train()\n",
    "    for ep in range(1, epochs+1):\n",
    "        run, n = 0.0, 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.view(xb.size(0), -1).to(device); yb = yb.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb); loss = F.cross_entropy(logits, yb)\n",
    "            loss.backward(); opt.step()\n",
    "            run += loss.item()*xb.size(0); n += xb.size(0)\n",
    "        print(f\"epoch {ep}: train_loss={run/n:.4f}\")\n",
    "\n",
    "# ----- Baseline -----\n",
    "class BaselineMLP(nn.Module):\n",
    "    def __init__(self, d, hidden, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, d)\n",
    "        self.head = nn.Linear(d, num_classes)\n",
    "        self.act = nn.GELU()\n",
    "        for m in [self.fc1, self.fc2, self.head]:\n",
    "            nn.init.xavier_uniform_(m.weight); nn.init.zeros_(m.bias)\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc1(x)); x = self.act(self.fc2(x)); return self.head(x)\n",
    "\n",
    "# ----- FSNN-MoM (two CPU/GPU-friendly variants) -----\n",
    "class FSNNMoMTopK(nn.Module):\n",
    "    \"\"\"\n",
    "    True top-k compute: we gather only the selected experts' weights/biases and\n",
    "    compute them. Robust gather handles any rank (W1: [M,d,h], W2: [M,h,d], b: [M,*]).\n",
    "    \"\"\"\n",
    "    def __init__(self, d, num_classes, M=6, k=2, h=64):\n",
    "        super().__init__(); assert 1 <= k <= M\n",
    "        self.M, self.k, self.d, self.h = M, k, d, h\n",
    "        self.W1 = nn.Parameter(torch.empty(M, d, h));   self.b1 = nn.Parameter(torch.zeros(M, h))\n",
    "        self.W2 = nn.Parameter(torch.empty(M, h, d));   self.b2 = nn.Parameter(torch.zeros(M, d))\n",
    "        nn.init.xavier_uniform_(self.W1); nn.init.xavier_uniform_(self.W2)\n",
    "        self.router = nn.Linear(d, M)\n",
    "        self.head   = nn.Linear(d, num_classes)\n",
    "        nn.init.xavier_uniform_(self.router.weight); nn.init.zeros_(self.router.bias)\n",
    "        nn.init.xavier_uniform_(self.head.weight);   nn.init.zeros_(self.head.bias)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def _gather_expert(self, t, topi):\n",
    "        # t: [M, ...], topi: [B, k]; returns gathered [B, k, ...] along t's first dim\n",
    "        B, k = topi.shape\n",
    "        # Expand t over batch: [B, M, ...]\n",
    "        t_exp = t.unsqueeze(0).expand(B, *t.shape)\n",
    "        # Build index with enough singleton dims to match t_exp\n",
    "        add_dims = t.dim() - 1\n",
    "        idx = topi.view(B, k, *([1]*add_dims)).expand(B, k, *t.shape[1:])\n",
    "        return t_exp.gather(dim=1, index=idx)\n",
    "\n",
    "    def forward(self, x):             # x: [B, d]\n",
    "        B = x.size(0)\n",
    "        logits = self.router(x)       # [B, M]\n",
    "        topv, topi = torch.topk(logits, k=self.k, dim=-1)   # [B, k]\n",
    "        weights = torch.softmax(topv, dim=-1)               # [B, k]\n",
    "\n",
    "        W1_sel = self._gather_expert(self.W1, topi)  # [B, k, d, h]\n",
    "        b1_sel = self._gather_expert(self.b1, topi)  # [B, k, h]\n",
    "        W2_sel = self._gather_expert(self.W2, topi)  # [B, k, h, d]\n",
    "        b2_sel = self._gather_expert(self.b2, topi)  # [B, k, d]\n",
    "\n",
    "        y1 = torch.einsum('bd,bkdh->bkh', x, W1_sel) + b1_sel\n",
    "        y1 = self.act(y1)\n",
    "        y  = torch.einsum('bkh,bkhd->bkd', y1, W2_sel) + b2_sel\n",
    "        y  = (y * weights.unsqueeze(-1)).sum(dim=1)         # [B, d]\n",
    "        return self.head(y)\n",
    "\n",
    "class FSNNMoMAll(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute-all variant: compute all M experts in one batched matmul, then mask to top-k.\n",
    "    This can be faster on CPU for small M because it avoids gathers.\n",
    "    \"\"\"\n",
    "    def __init__(self, d, num_classes, M=6, k=2, h=64):\n",
    "        super().__init__(); assert 1 <= k <= M\n",
    "        self.M, self.k, self.d, self.h = M, k, d, h\n",
    "        self.W1 = nn.Parameter(torch.empty(M, d, h));   self.b1 = nn.Parameter(torch.zeros(M, h))\n",
    "        self.W2 = nn.Parameter(torch.empty(M, h, d));   self.b2 = nn.Parameter(torch.zeros(M, d))\n",
    "        nn.init.xavier_uniform_(self.W1); nn.init.xavier_uniform_(self.W2)\n",
    "        self.router = nn.Linear(d, M); self.head = nn.Linear(d, num_classes)\n",
    "        nn.init.xavier_uniform_(self.router.weight); nn.init.zeros_(self.router.bias)\n",
    "        nn.init.xavier_uniform_(self.head.weight);   nn.init.zeros_(self.head.bias)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):             # x: [B, d]\n",
    "        B = x.size(0)\n",
    "        logits = self.router(x)       # [B, M]\n",
    "        topv, topi = torch.topk(logits, k=self.k, dim=-1)\n",
    "        weights = torch.softmax(topv, dim=-1)               # [B, k]\n",
    "        # Compute all experts: [B, M, d]\n",
    "        y1 = torch.einsum('bd,mdh->bmh', x, self.W1) + self.b1\n",
    "        y1 = self.act(y1)\n",
    "        y_all = torch.einsum('bmh,mhd->bmd', y1, self.W2) + self.b2\n",
    "        # Build mask with top-k weights\n",
    "        mask = torch.zeros(B, self.M, device=x.device)\n",
    "        mask.scatter_(1, topi, weights)\n",
    "        y = (y_all * mask.unsqueeze(-1)).sum(dim=1)         # [B, d]\n",
    "        return self.head(y)\n",
    "\n",
    "# ----- comparison harness -----\n",
    "def run_case(title, baseline_hidden, mom_cfg, epochs=2, variant=\"topk\"):\n",
    "    print(f\"\\n=== {title} | FSNN variant: {variant} ===\")\n",
    "    # Baseline\n",
    "    base = BaselineMLP(D, baseline_hidden, NUM_CLASSES).to(device)\n",
    "    train(base, epochs=epochs, lr=1e-3)\n",
    "    lb, ab = evaluate(base); p50b, p90b, _ = latency_bs1(base); pb = count_params(base)\n",
    "    print(f\"Baseline: params={pb:,}  acc={ab:.4f}  p50={p50b*1e3:.2f} ms  p90={p90b*1e3:.2f} ms\")\n",
    "\n",
    "    # FSNN\n",
    "    FSNN = FSNNMoMTopK if variant==\"topk\" else FSNNMoMAll\n",
    "    fsnn = FSNN(D, NUM_CLASSES, **mom_cfg).to(device)\n",
    "    train(fsnn, epochs=epochs, lr=1e-3)\n",
    "    lf, af = evaluate(fsnn); p50f, p90f, _ = latency_bs1(fsnn); pf = count_params(fsnn)\n",
    "    k, h = mom_cfg['k'], mom_cfg['h']\n",
    "    print(f\"FSNN  : params={pf:,}  acc={af:.4f}  p50={p50f*1e3:.2f} ms  p90={p90f*1e3:.2f} ms  (active width ~ {k*h})\")\n",
    "\n",
    "# 1) PARAM-PARITY: match baseline hidden to M*h, but FSNN computes only k experts\n",
    "run_case(\"Param-Parity (sparse compute)\", baseline_hidden=384,\n",
    "         mom_cfg=dict(M=6, k=2, h=64), epochs=2, variant=\"topk\")\n",
    "\n",
    "# For CPU, also try the compute-all masking variant:\n",
    "run_case(\"Param-Parity (sparse compute)\", baseline_hidden=384,\n",
    "         mom_cfg=dict(M=6, k=2, h=64), epochs=2, variant=\"all\")\n",
    "\n",
    "# 2) COMPUTE-PARITY: same active compute (k*h = 128) vs baseline hidden=128\n",
    "run_case(\"Compute-Parity (richer capacity)\", baseline_hidden=128,\n",
    "         mom_cfg=dict(M=6, k=2, h=64), epochs=2, variant=\"topk\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
